<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="language" content="english">
<title>Efficient Video Instance Segmentation via Tracklet Query and Proposal</title>
<meta name="description" content="Deep Plastic Surgery">
<meta name="author" content="Jialian Wu">
	<link rel="shortcut icon" href="../images/ub.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" href="css/project.css">
</head>
	<script> 
	function coming_soon()
	{
		alert("We are cleaning up our code to make it more simple and readable");
	}
	</script> 

<body>
<div id="main">
  
	<div class="content"><br>
		<div class="title">
			<h1>Efficient Video Instance Segmentation via Tracklet Query and Proposal </h1>
		</div>
		<div class="authors" style="width:95%">
			<div class='author'>
				 <A>Jialian Wu,</A>
		    </div>
			<div class='author'>
				<A>Sudhir Yarram,</A>
			</div>
			<div class='author'>
				<A>Hui Liang,</A>
			</div>		
			<div class='author'>
				<A>Tian Lan,</A>
			</div>
			<div class='author'>
				<A>Junsong Yuan,</A>
			</div>
			<div class='author'>
				<A>Jayan Eledath,</A>
			</div>
			<div class='author'>
				<A>G&eacuterard Medioni</A>
			</div>
		</div>
		<br>
		<div class="affiliation">
		<A><sup>1</sup>State University of New York at Buffalo &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; <sup>2</sup>Amazon</A>
		</div>
		<br>
		<div class="publication" style="width:95%">
			<A>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</A>
		</div>
		<br>

<!--		<div style="width:1200px;">-->
<!--			<div style="position:relative; left:-200px; float: left">-->
<!--				<iframe width="560" height="315" src="https://www.youtube.com/embed/rfzODwpwqso" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->

<!--			</div>-->
<!--			<div style="position:relative; right:260px;float: right">-->
<!--				<iframe width="560" height="315" src="https://www.youtube.com/embed/rfzODwpwqso" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
<!--			</div>-->
<!--			<div style="clear:both;"></div>-->
<!--		</div>-->

		<div class="video" style="width:95%">
		<iframe width="700" height="432" src="https://www.youtube.com/embed/sSPMzgtMKCE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		</div>
<!--		<div class="video" style="width:95%">-->
<!--		<iframe width="560" height="315" src="https://www.youtube.com/embed/rfzODwpwqso" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
<!--		</div>-->
<!--		<div class="video" style="width:95%">-->
<!--			<iframe width="560" height="315" src="https://www.youtube.com/embed/rfzODwpwqso" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
<!--		</div>-->
		<div class="papercode" style="width:95%">
			<br>
			<div class='paper'>
			<a target="_blank" href="https://arxiv.org/pdf/2203.01853.pdf"><img src="../images/pdf.png"  title="name" style="width:4%;vertical-align:middle;padding: 0 5px 10px 0"/>Paper</a>
			</div>
			<div class='paper'>
			<a target="_blank" ><img src="../images/github2.png" title="name" style="width:5%;vertical-align:middle;padding: 0 5px 12px 0"/>Code(Coming)</a>
			</div>
<!--			<div class='paper'>-->
<!--				<a target="_blank" href="https://www.bilibili.com/video/BV12U4y1p7wg"><img src="../images/bilibili.jpg" title="name" style="width:5%;vertical-align:middle;padding: 0 5px 12px 0"/>bilibili</a>-->
<!--			</div>-->
		</div>

		<div class="abstract_sec">
			<h2>Abstract</h2>
			<div class='desp'>
				<p style="text-align:justify">
					Video Instance Segmentation (VIS) aims to simultaneously classify, segment, and track multiple object instances in
					videos. This paper proposes EfficientVIS,
					a fully end-to-end framework with efficient training and inference. At the core are tracklet query and tracklet proposal
					that associate and segment regions-of-interest (RoIs) across
					space and time by an iterative query-video interaction. We
					further propose a correspondence learning that makes tracklets linking between clips end-to-end learnable. Compared
					to VisTR, EfficientVIS requires 15x fewer training epochs
					while achieving state-of-the-art accuracy on the YouTubeVIS benchmark. Meanwhile, our method enables whole video
					instance segmentation in a single end-to-end pass without
					data association at all.
					</p>
			</div>
		</div>

		<div class="abstract_sec">
			<h2>Framework</h2>
			<div class="images">
		  		<img src='EfficientVIS/EfficientVIS.jpg' width='100%' alt="Teaser" >
		  		<p style="text-align: justify">Figure 1. EfficientVIS architecture. EfficientVIS performs VIS clip-by-clip where the above figure illustrates how it works in one clip.
			</div>

		
<!--		<div class="download_sec">-->
<!--			<h2>Resources</h2>-->
<!--			<div>-->
<!--				<li><strong>Paper</strong>: <a href="https://arxiv.org/abs/2001.02890">arXiv</a></li>-->
<!--				<li><strong>Supplementary Material</strong>: <a href="./DPS/files/DeepPlasticSurgery-supp.pdf">PDF</a> (7.53MB), <a href="https://drive.google.com/file/d/1W5nCrtEwqtI9vXandHey2hVVb7hB9xC7/view">MP4</a> (36.1MB)</li>-->
<!--				<li><strong>Human-Drawn Facial Sketches</strong>: <a href="./DPS/files/human-drawn_facial_sketches.zip">ZIP</a> (11.1MB, our collected 30 facial sketches and results by [3], [4] and our model.)</li>-->
<!--				<li><strong>Released Code</strong>: <a href="https://github.com/TAMU-VITA/DeepPS">Pytorch implementation</a></li>-->
<!--			</div>-->
<!--		</div>-->

<!--		<div class='citation_sec'>-->
<!--			<h2>Citation</h2>-->
<!--			<p class='bibtex'>@inproceedings{Wu2021TraDeS,-->
<!-- title={Track to Detect and Segment: An Online Multi-Object Tracker},-->
<!-- author={Wu, Jialian and Cao, Jiale and Song, Liangchen and Wang, Yu and Yang, Ming and Yuan, Junsong},-->
<!-- booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},-->
<!-- year={2021}-->
<!--}</p>-->
<!--		</div>-->

		<div class="experiments_sec">
			<h2>Selected Results</h2>
			<div id="images">
				<img src="EfficientVIS/results.png" alt="" width="100%" >
<!--				<P style="text-align: justify">Figure 3. Comparison with state-of-the-art methods on face edting. (a) Input photos, masks and sketches. (b) DeepFillv2 [1]. (c) SC-FEGAN [2].  (d) Our results with <em>l</em>=0. (e) Our results with <em>l</em>=1. (f) SC-FEGAN using our refined sketches as input.</P>-->
			</div>
<!--			<div id="images">-->
<!--				<img src="DPS/figures/compare2.jpg" alt="" width="100%" >		-->
<!--				<P style="text-align: justify">Figure 4. Comparison with state-of-the-art methods on face synthesis. (a) Input human-drawn sketches. (b) BicycleGAN [3]. (c) pix2pixHD [4].  (d) pix2pix [5]. (e) Our results with <em>l</em>=1. (f) pix2pixHD using our refined sketches as input.</P>	-->
<!--			</div>			-->
			
		</div>
		
<!--		<div class="reference_sec">-->
<!--		<h2>Reference</h2>-->
<!--		  <div class="bib" style="text-align: justify">-->
<!--&lt;!&ndash;		    <p>[1] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, T.S. Huang. Free-form image inpainting with gated convolution. ICCV 2019.</p>&ndash;&gt;-->
<!--&lt;!&ndash;		    <p>[2] Y. Jo, J. Park. SC-FEGAN: Face editing generative adversarial network with userâ€™s sketch and color. ICCV 2019.</p>&ndash;&gt;-->
<!--&lt;!&ndash;		  	<p>[3] J.Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A.A. Efros, O. Wang, E. Shechtman. Toward multimodal image-to-image translation. NeurIPS 2017.</p>&ndash;&gt;-->
<!--&lt;!&ndash;			<p>[4] T.C. Wang, M.Y. Liu, J.Y. Zhu, A. Tao, J. Kautz, B. Catanzaro. High-resolution image synthesis and semantic manipulation with conditional GANs. CVPR 2018.</p>&ndash;&gt;-->
<!--&lt;!&ndash;			<p>[5] P. Isola, J.Y. Zhu, T. Zhou, A.A. Efros. Image-to-image translation with conditional adversarial networks. CVPR 2017.</p>&ndash;&gt;-->
<!--		  </div>-->
<!--	</div>-->

			<div class='citation_sec'>
				<h2>Citation</h2>
				<p class='bibtex'>@inproceedings{Wu2022EfficientVIS,
title={Efficient Video Instance Segmentation via Tracklet Query and Proposal},
author={Wu, Jialian and Yarram, Sudhir and Liang, Hui and Lan, Tian and Yuan, Junsong and Eledath, Jayan and Medioni, Gerard},
booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
year={2022}}</p>
		
		
		<br></br>
		<p class="banner"align="center">Return to <a href="https://jialianwu.com/">Homepage</a></p>


<!--		<p class="banner"align="center">Powered by <a href="https://williamyang1991.github.io/">Shuai Yang</a></p>-->
  </div>
</div>
</body>
</html>
